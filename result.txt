1. preparation about NVMe SSD
   (1) mount nvme as "ordered" mode.
   # sudo mount -t ext4 -o data=ordered /dev/nvme0n1 /mnt
   
   (2) Easy througput tests for read and write.
   <Read>
   $ time dd if=/mnt/testfile of=/dev/null bs=1024k iflag=direct
   8192+0 records in
   8192+0 records out
   8589934592 bytes (8.6 GB, 8.0 GiB) copied, 5.22648 s, 1.6 GB/s

   real    0m5.229s
   user    0m0.043s
   sys     0m1.273s

   <Write>
   $ time dd if=/dev/zero of=/mnt/testfile bs=1024k count=8192 oflag=direct
   8192+0 records in
   8192+0 records out
   8589934592 bytes (8.6 GB, 8.0 GiB) copied, 10.6055 s, 810 MB/s

   real    0m10.610s
   user    0m0.043s
   sys     0m2.849s
   
   As you know PCIe Gen3 x4 = 8GT/s*4*(128/130)/8 = 3.9GB/s, But it seems limited by NVMe itself. 
   - Sequential Read (MB/s)	1,950MB/s
   - Sequential Write (MB/s)	1,250MB/s
   See also the https://www.klevv.com/kjp/products_details/ssd/Klevv_Cras_C710_SSD.php.
   
2. Check if gds is ready by using gdsio_verify.
   $ gdsio_verify -d 0 -f /mnt/test -o 0 -s 1M -n 1 -m 0
   gpu index :0,file :/mnt/test, gpu buffer alignment :0, gpu buffer offset :0, gpu devptr offset :0, file offset :0, io_requested :1048576, io_chunk_size :1048576, bufregister :true, sync :0, nr ios :1, 
   fsync :0, 
   address = 0x1
   Data Verification Success

3. Seq Read Throughput
(1) Storage->CPU
    This value means that NVMe's seq-read throughput from NVMe to system memory is 1.36GB/s.
    
    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 1 -I 0 -T 10
    IoType: READ XferType: CPUONLY Threads: 1 DataSetSize: 14655488/1048576(KiB) IOSize: 1024(KiB) Throughput: 1.368340 GiB/sec, Avg_Latency: 713.614380 usecs ops: 14312 total_time 10.214245 secs

(2) Storage->CPU->GPU
    Bounce buffer occuring from NVMe to GPU memory and it was 1.21GB/s as you can see below:

    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 2 -I 0 -T 10
    IoType: READ XferType: CPU_GPU Threads: 1 DataSetSize: 12558336/1048576(KiB) IOSize: 1024(KiB) Throughput: 1.210532 GiB/sec, Avg_Latency: 806.636171 usecs ops: 12264 total_time 9.893633 secs

(3) Storage -> GPU (GDS)
    GDS eliminates bounce buffer so that it can read at 1.38GB/s.

    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 0 -I 0 -T 10
    IoType: READ XferType: GPUD Threads: 1 DataSetSize: 14655488/1048576(KiB) IOSize: 1024(KiB) Throughput: 1.384300 GiB/sec, Avg_Latency: 705.381428 usecs ops: 14312 total_time 10.096484 secs

4. Seq Write Throughput
(1) Storage->CPU
    0.98GB/s from NVMe to system memory.
    
    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 1 -I 1 -T 10
    IoType: WRITE XferType: CPUONLY Threads: 1 DataSetSize: 10461184/1048576(KiB) IOSize: 1024(KiB) Throughput: 0.983957 GiB/sec, Avg_Latency: 992.411609 usecs ops: 10216 total_time 10.139222 secs

(2) Storage->CPU->GPU
    Bounce buffer occuring from GPU memory to NVMe and it was 0.95GB/s as you can see below:
    
    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 2 -I 1 -T 10
    IoType: WRITE XferType: CPU_GPU Threads: 1 DataSetSize: 9412608/1048576(KiB) IOSize: 1024(KiB) Throughput: 0.958548 GiB/sec, Avg_Latency: 1018.719756 usecs ops: 9192 total_time 9.364754 secs

(3) Storage -> GPU (GDS)
    GDS eliminates bounce buffer so that it can write at 0.97GB/s.

    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 0 -I 1 -T 10
    IoType: WRITE XferType: GPUD Threads: 1 DataSetSize: 10461184/1048576(KiB) IOSize: 1024(KiB) Throughput: 0.973278 GiB/sec, Avg_Latency: 1003.289644 usecs ops: 10216 total_time 10.250480 secs
    
    
5. Rand Read Throughput
(1) Storage->CPU
    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 2 -I 2 -T 10
    IoType: RANDREAD XferType: CPU_GPU Threads: 1 DataSetSize: 10461184/1048576(KiB) IOSize: 1024(KiB) Throughput: 0.949162 GiB/sec, Avg_Latency: 1028.752937 usecs ops: 10216 total_time 10.510911 secs
    
(2) Storage->CPU->GPU
    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 1 -I 2 -T 10
    IoType: RANDREAD XferType: CPUONLY Threads: 1 DataSetSize: 11509760/1048576(KiB) IOSize: 1024(KiB) Throughput: 1.084089 GiB/sec, Avg_Latency: 900.726157 usecs ops: 11240 total_time 10.125149 secs

(3) Storage -> GPU (GDS)
    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 0 -I 2 -T 10
IoType: RANDREAD XferType: GPUD Threads: 1 DataSetSize: 11509760/1048576(KiB) IOSize: 1024(KiB) Throughput: 1.090562 GiB/sec, Avg_Latency: 895.391459 usecs ops: 11240 total_time 10.065048 secs

6. Rand Write Throughput
(1) Storage->CPU
    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 2 -I 3 -T 20
IoType: RANDWRITE XferType: CPU_GPU Threads: 1 DataSetSize: 15704064/1048576(KiB) IOSize: 1024(KiB) Throughput: 0.768765 GiB/sec, Avg_Latency: 1270.247783 usecs ops: 15336 total_time 19.481319 secs

(2) Storage->CPU->GPU
    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 1 -I 3 -T 20
IoType: RANDWRITE XferType: CPUONLY Threads: 1 DataSetSize: 12582912/1048576(KiB) IOSize: 1024(KiB) Throughput: 0.622306 GiB/sec, Avg_Latency: 1568.232747 usecs ops: 12288 total_time 19.283117 secs

(3) Storage -> GPU (GDS)
    $ gdsio -f /mnt/test1G -d 0 -n 0 -w 1 -s 1G -x 0 -I 3 -T 20
IoType: RANDWRITE XferType: GPUD Threads: 1 DataSetSize: 19898368/1048576(KiB) IOSize: 1024(KiB) Throughput: 0.918591 GiB/sec, Avg_Latency: 1063.049094 usecs ops: 19432 total_time 20.658341 secs
